# Testing Frameworks for IoT Telemetry Systems

**Date**: December 28, 2025  
**Purpose**: Senior-level interview preparation - Testing strategy comparison

---

## ü§ñ ROBOT Framework (Your Experience)

### What is ROBOT Framework?
- **Type**: Keyword-driven test automation framework
- **Language**: Python-based with tabular test syntax
- **Best For**: Acceptance testing, BDD, IoT device integration
- **Industry Use**: Nokia, ABB, Ericsson (heavy IoT users)

### ROBOT Framework Strengths
‚úÖ **Human-Readable Tests**: Business stakeholders can read/write tests
```robot
*** Test Cases ***
Verify Temperature Sensor Reading
    Connect To Device    sensor-001
    ${temp}=    Read Temperature
    Should Be Between    ${temp}    20.0    30.0
    Disconnect From Device
```

‚úÖ **Rich Ecosystem**: 500+ libraries (HTTP, MQTT, Modbus, Redis)
‚úÖ **HTML Reports**: Beautiful test reports with logs, screenshots
‚úÖ **Parallel Execution**: Run tests across multiple devices
‚úÖ **CI/CD Integration**: Jenkins, GitLab, GitHub Actions

### ROBOT Framework for TelemetryHub
```robot
*** Settings ***
Library    RequestsLibrary
Library    RedisLibrary
Library    Collections

*** Variables ***
${GATEWAY_URL}    http://localhost:8080
${REDIS_HOST}     localhost
${REDIS_PORT}     6379

*** Test Cases ***
Test Telemetry Ingestion Pipeline
    [Documentation]    End-to-end test: Device -> Gateway -> Redis -> Processor
    [Tags]    integration    e2e
    
    # 1. Send telemetry to gateway
    ${payload}=    Create Dictionary    
    ...    device_id=sensor-001
    ...    temperature=25.5
    ...    unit=celsius
    
    ${response}=    POST    ${GATEWAY_URL}/telemetry    json=${payload}
    Should Be Equal As Numbers    ${response.status_code}    200
    
    # 2. Verify data in Redis queue
    Connect To Redis    ${REDIS_HOST}    ${REDIS_PORT}
    ${queue_length}=    Get Redis List Length    telemetry_queue
    Should Be Greater Than    ${queue_length}    0
    
    # 3. Verify processor consumed data
    Sleep    2s    # Wait for processing
    ${processed}=    Get Redis Key    processed:sensor-001
    Should Contain    ${processed}    temperature
    
    [Teardown]    Disconnect From Redis

Test High Load Scenario
    [Documentation]    Send 10,000 events and verify throughput
    [Tags]    performance    stress
    
    ${start_time}=    Get Time    epoch
    
    FOR    ${i}    IN RANGE    10000
        ${payload}=    Create Telemetry Payload    sensor-${i}
        POST Without Waiting    ${GATEWAY_URL}/telemetry    json=${payload}
    END
    
    Wait Until All Requests Complete
    ${end_time}=    Get Time    epoch
    ${duration}=    Evaluate    ${end_time} - ${start_time}
    
    # Verify throughput > 1000 events/sec
    ${throughput}=    Evaluate    10000 / ${duration}
    Should Be Greater Than    ${throughput}    1000
    
Test Protobuf Serialization Performance
    [Documentation]    Compare Protobuf vs JSON performance
    [Tags]    benchmark
    
    ${sample}=    Create Sample Data
    
    # Protobuf serialization
    ${proto_start}=    Get Time    epoch
    FOR    ${i}    IN RANGE    10000
        ${binary}=    Serialize To Protobuf    ${sample}
    END
    ${proto_duration}=    Calculate Duration
    
    # JSON serialization
    ${json_start}=    Get Time    epoch
    FOR    ${i}    IN RANGE    10000
        ${json}=    Convert To JSON    ${sample}
    END
    ${json_duration}=    Calculate Duration
    
    # Protobuf should be 10x faster
    ${speedup}=    Evaluate    ${json_duration} / ${proto_duration}
    Should Be Greater Than    ${speedup}    8    # At least 8x faster
```

---

## üÜö Modern Alternatives Comparison

### 1. **pytest** (Python - Recommended for C++ Projects)

**Why Choose pytest?**
- ‚úÖ Industry standard for Python testing (70%+ market share)
- ‚úÖ Excellent C++ integration via pybind11, ctypes
- ‚úÖ Fixtures for setup/teardown (better than ROBOT)
- ‚úÖ Parametrized tests (data-driven testing)
- ‚úÖ Rich plugin ecosystem (pytest-timeout, pytest-xdist, pytest-cov)

**TelemetryHub Integration**:
```python
# tests/integration/test_telemetry_pipeline.py
import pytest
import requests
import redis
import subprocess
import time

@pytest.fixture(scope="module")
def gateway_server():
    """Start gateway server for tests"""
    proc = subprocess.Popen(["./gateway_app"])
    time.sleep(2)  # Wait for startup
    yield "http://localhost:8080"
    proc.terminate()

@pytest.fixture
def redis_client():
    """Redis connection with cleanup"""
    client = redis.Redis(host='localhost', port=6379)
    yield client
    client.flushdb()  # Clean up after test

def test_telemetry_ingestion(gateway_server, redis_client):
    """Test full ingestion pipeline"""
    # Send telemetry
    payload = {
        "device_id": "sensor-001",
        "temperature": 25.5,
        "unit": "celsius"
    }
    response = requests.post(f"{gateway_server}/telemetry", json=payload)
    assert response.status_code == 200
    
    # Verify in Redis
    queue_length = redis_client.llen("telemetry_queue")
    assert queue_length > 0
    
    # Verify processed
    time.sleep(2)
    processed = redis_client.get("processed:sensor-001")
    assert processed is not None

@pytest.mark.parametrize("device_id,temp,unit", [
    ("sensor-001", 25.5, "celsius"),
    ("sensor-002", 75.2, "fahrenheit"),
    ("sensor-003", 298.15, "kelvin"),
])
def test_multiple_devices(gateway_server, device_id, temp, unit):
    """Test different device types"""
    payload = {"device_id": device_id, "temperature": temp, "unit": unit}
    response = requests.post(f"{gateway_server}/telemetry", json=payload)
    assert response.status_code == 200

@pytest.mark.performance
def test_high_throughput(gateway_server, benchmark):
    """Benchmark ingestion rate"""
    def send_telemetry():
        payload = {"device_id": "sensor-perf", "temperature": 25.5}
        requests.post(f"{gateway_server}/telemetry", json=payload)
    
    # pytest-benchmark: Run function multiple times
    result = benchmark(send_telemetry)
    
    # Verify > 1000 requests/sec
    assert result.stats['mean'] < 0.001  # < 1ms per request
```

**pytest Advantages over ROBOT**:
- ‚úÖ Native Python (better IDE support, debugging)
- ‚úÖ Type hints with mypy (catch errors early)
- ‚úÖ Async/await support (test async C++ code)
- ‚úÖ Better parametrization (test same code with 100 inputs)

---

### 2. **Catch2** (C++ Native - Best for Unit Tests)

**Why Choose Catch2?**
- ‚úÖ Header-only, easy to integrate
- ‚úÖ BDD-style syntax (SCENARIO/GIVEN/WHEN/THEN)
- ‚úÖ Fast compilation (vs GoogleTest)
- ‚úÖ Beautiful output (colored, grouped)

**Example**:
```cpp
#include <catch2/catch_test_macros.hpp>
#include "task_queue.h"

SCENARIO("TaskQueue handles high-priority tasks first") {
    GIVEN("A queue with mixed priority tasks") {
        TaskQueue queue;
        
        WHEN("Tasks are enqueued out of order") {
            queue.enqueue(Task("low", TaskPriority::LOW));
            queue.enqueue(Task("high", TaskPriority::HIGH));
            queue.enqueue(Task("medium", TaskPriority::MEDIUM));
            
            THEN("High-priority task is dequeued first") {
                auto task = queue.dequeue();
                REQUIRE(task.has_value());
                REQUIRE(task->priority == TaskPriority::HIGH);
            }
        }
    }
}
```

**Catch2 vs GoogleTest**:
| Feature | Catch2 | GoogleTest |
|---------|--------|------------|
| Header-only | ‚úÖ Yes | ‚ùå No (needs linking) |
| BDD syntax | ‚úÖ SCENARIO/GIVEN | ‚ùå TEST() only |
| Compilation speed | ‚úÖ Fast | ‚ö†Ô∏è Slower |
| Industry adoption | ‚ö†Ô∏è Growing | ‚úÖ Dominant |
| CMake integration | ‚úÖ Easy | ‚úÖ Easy |

---

### 3. **Locust** (Load Testing - Python)

**Why Choose Locust?**
- ‚úÖ Distributed load testing (1M+ concurrent users)
- ‚úÖ Web UI for monitoring (real-time graphs)
- ‚úÖ Python scripting (easy to customize)

**Example**:
```python
from locust import HttpUser, task, between

class TelemetryUser(HttpUser):
    wait_time = between(0.1, 0.5)  # Simulate device sending every 100-500ms
    
    @task
    def send_telemetry(self):
        payload = {
            "device_id": f"sensor-{self.user_id}",
            "temperature": 25.0 + (self.user_id % 10),
            "unit": "celsius"
        }
        self.client.post("/telemetry", json=payload)
    
    @task(2)  # Run this twice as often
    def health_check(self):
        self.client.get("/health")

# Run: locust -f tests/load/telemetry_load.py --users 10000 --spawn-rate 100
```

---

### 4. **k6** (Modern Load Testing - Go/JavaScript)

**Why Choose k6?**
- ‚úÖ Better than JMeter (modern, scriptable)
- ‚úÖ JavaScript DSL (familiar syntax)
- ‚úÖ Built-in metrics (p95, p99 latency)
- ‚úÖ Grafana integration (beautiful dashboards)

**Example**:
```javascript
// tests/load/telemetry_load.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export let options = {
    stages: [
        { duration: '30s', target: 100 },   // Ramp up to 100 users
        { duration: '1m', target: 1000 },   // Ramp up to 1000 users
        { duration: '30s', target: 0 },     // Ramp down
    ],
    thresholds: {
        http_req_duration: ['p(95)<200'],   // 95% of requests < 200ms
        http_req_failed: ['rate<0.01'],     // Error rate < 1%
    },
};

export default function() {
    let payload = JSON.stringify({
        device_id: `sensor-${__VU}`,  // Virtual User ID
        temperature: 25.5,
        unit: 'celsius',
    });
    
    let res = http.post('http://localhost:8080/telemetry', payload, {
        headers: { 'Content-Type': 'application/json' },
    });
    
    check(res, {
        'status is 200': (r) => r.status === 200,
        'response time < 200ms': (r) => r.timings.duration < 200,
    });
    
    sleep(1);  // 1 request per second per VU
}

// Run: k6 run --vus 1000 --duration 2m tests/load/telemetry_load.js
```

---

## üèÜ Recommendation for TelemetryHub

### Testing Strategy (Multi-Layer)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 1: Unit Tests (C++ - GoogleTest/Catch2)              ‚îÇ
‚îÇ - TaskQueue priority ordering                               ‚îÇ
‚îÇ - ProtoAdapter serialization                                ‚îÇ
‚îÇ - RedisClient operations                                    ‚îÇ
‚îÇ - Fast: ~100ms for 1000 tests                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 2: Integration Tests (Python - pytest)               ‚îÇ
‚îÇ - Gateway -> Redis -> Processor pipeline                    ‚îÇ
‚îÇ - End-to-end telemetry flow                                ‚îÇ
‚îÇ - Database interactions                                     ‚îÇ
‚îÇ - Moderate: ~10s for 50 tests                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 3: Load Tests (k6 or Locust)                         ‚îÇ
‚îÇ - 10,000 concurrent devices                                 ‚îÇ
‚îÇ - Throughput validation (50k events/sec)                   ‚îÇ
‚îÇ - Latency percentiles (p95, p99)                           ‚îÇ
‚îÇ - Slow: ~2 minutes per test                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 4: Acceptance Tests (ROBOT Framework - Optional)     ‚îÇ
‚îÇ - Business-readable scenarios                               ‚îÇ
‚îÇ - Device certification testing                              ‚îÇ
‚îÇ - Vendor-specific protocols (Modbus, MQTT)                 ‚îÇ
‚îÇ - For non-technical stakeholders                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### My Recommendation

**For TelemetryHub specifically**:

1. **Primary: GoogleTest (keep what we have)**
   - ‚úÖ Already integrated
   - ‚úÖ Industry standard for C++
   - ‚úÖ 20+ tests already written

2. **Add: pytest for integration tests**
   - ‚úÖ Test full pipeline (Gateway -> Redis -> Processor)
   - ‚úÖ Easy to write, fast to execute
   - ‚úÖ Better than ROBOT for programmatic tests

3. **Add: k6 for load testing**
   - ‚úÖ Modern, better than JMeter
   - ‚úÖ Grafana integration (beautiful dashboards)
   - ‚úÖ Cloud-ready (k6 Cloud for distributed testing)

4. **Optional: ROBOT for acceptance testing**
   - ‚úÖ Use ONLY if non-technical stakeholders write tests
   - ‚úÖ Good for device certification (test 100 sensor models)
   - ‚ö†Ô∏è Overkill for pure backend testing

---

## üéØ Interview Talking Points

### "Why not ROBOT Framework?"

**Good Answer**:
"I have experience with ROBOT Framework from my IoT work at [previous company]. It's excellent for acceptance testing when non-technical stakeholders need to write tests, and the keyword-driven syntax makes it easy to create readable test scenarios.

However, for TelemetryHub, I chose **GoogleTest + pytest + k6** because:

1. **GoogleTest**: Industry standard for C++ (same as Google, LLVM, Qt). Fast unit tests, good IDE integration.

2. **pytest**: More flexible than ROBOT for programmatic integration tests. Better debugging, type hints with mypy, async/await support.

3. **k6**: Modern load testing with built-in p95/p99 metrics and Grafana integration. Better than JMeter for cloud-native systems.

I'd consider ROBOT if we needed:
- Device certification testing (100+ vendor-specific sensors)
- Business analysts writing acceptance tests
- Keyword libraries for Modbus, OPC-UA, or other IoT protocols

But for pure backend API testing, pytest is more maintainable."

---

## üìö Resources

### ROBOT Framework
- Official Docs: https://robotframework.org/
- IoT Libraries: https://github.com/robotframework/MQTTLibrary
- Best Practices: https://github.com/robotframework/HowToWriteGoodTestCases

### pytest
- Official Docs: https://docs.pytest.org/
- pytest-benchmark: https://pytest-benchmark.readthedocs.io/
- Integration with C++: https://pybind11.readthedocs.io/

### k6
- Official Docs: https://k6.io/docs/
- Examples: https://github.com/grafana/k6-example-scripts
- Grafana Cloud: https://grafana.com/products/cloud/k6/

### Catch2
- Official Docs: https://github.com/catchorg/Catch2
- Tutorial: https://github.com/catchorg/Catch2/blob/devel/docs/tutorial.md

---

**Conclusion**: Use **pytest** for integration tests, keep **GoogleTest** for unit tests, add **k6** for load testing. ROBOT is good but overkill for your use case.

**Interview Strategy**: Mention ROBOT experience but explain why you chose more modern tools. Shows you're not stuck in old ways!
